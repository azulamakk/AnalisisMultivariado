---
title: "Taller 2 - Transformaciones lineales de datos"
authors: Nico y Justo
date: "2023-08-25"
output:
  html_document: default
  pdf_document: default
---

## Arrancamos haciendo un mini EDA antes de meternos de lleno a la guia de ejercicios. 


```{r}
trade = read.delim("~/Universidad/Analisis Multivariado/datasets/trade.txt")
print(trade)
head(trade)
```
```{r}
summary(trade)
```
```{r}
plot(x=trade$imports, y=trade$exports)
```

## Arrancamos la guia!

Vamos a llamar a imports X1 y a exports X2 para mas facilidad en el manejo de la data

```{r}
names(trade) = c('X2','X1')

```


### a. Centrar los datos y graficarlos en un scatterplot.

```{r plot-data}
#centramos los datos(estandarizacion)

trade = scale(trade)
summary(trade)

#lo volvemos a transformar en un df luego de estandarizarlo
trade = as.data.frame(trade)
#scatterplot
plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados", xlab="Imports", ylab="Exports", pch=5, col="blue")
```


### b. Elegir una recta que pase sobre el origen para proyectar la nube de puntos. Agregarla al grafico anterior.

```{r plot-line}
#para que funcione hay que correr todo junto

plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados", xlab="Imports", ylab="Exports", pch=5, col="blue")
abline(a=0, b=1, col="red", lwd=2)
```


### c. A partir de la recta definida, escribir la ecuaci´on que permite transformar X1 y X2 a Y.

Dado que la línea es y = x (porque tiene una intersección de 0 y una pendiente de 1), la proyección de un punto (X1, X2) en esta línea será una combinación de X1 y X2 tal que Y = X1 + X2.

Conceptualmente: La proyección en la línea y = x implica que estamos sumando las contribuciones de X1 y X2 para obtener un único valor Y. Esta es una forma de reducir la dimensión de los datos, pasando de 2D a 1D.

Por lo tanto, la ecuación correcta para la proyección en la línea que has elegido es:

```{r}

# Y = X1 + X2

```


### d. Crear la matriz de datos X y un vector a, de norma 1, para poder realizar la transformaci´on anterior a todos los datos.

Para realizar la transformación anterior a todos los datos, necesitamos definir la matriz de datos de direccion X y un vector de direccion a de norma 1 que represente la dirección de proyección.

Dado que la línea de proyección es Y = X, la dirección de proyección es a lo largo de la diagonal. Un vector que apunte en esta dirección es [1,1]
Sin embargo, para que tenga norma 1, debemos normalizarlo.

Para normalizar un vector, lo dividimos por su norma (o longitud). La norma de un vector v se calcula como la raíz cuadrada de la suma de sus componentes al cuadrado.

```{r}

X = data.matrix(trade)

#matrix x
print(X)

a = c(1,1)

norma_a = sqrt(sum(a^2))

a = a / norma_a

#vector de direccion a normalizado 

a

```


La proyección de cada punto de X en la dirección de a se obtiene calculando el producto escalar entre el punto y a. Esto nos da un valor escalar para cada punto, que es su proyección en la línea.

```{r}

Y <- X %*% a


```
Conceptualmente: Al proyectar los datos en la dirección de a, estamos "colapsando" la información de 2D a 1D. Cada punto en el espacio 2D se mapea a un valor escalar en la línea definida por a.

Con estos pasos, habrás transformado todos tus datos según la dirección de proyección definida por la línea Y = X.

### e. Realizar un histograma y calcular la varianza muestral de la nueva variable Y.

```{r}

hist(Y)

var_y = var(Y)

var_y

```


### f. Definamos una nueva variable E que resuma el error de representacion de cada paıs, y que se obtenga al proyectar los datos en la direccion ortogonal a la recta elegida en b). Encontrar la pendiente y ordenada al origen de dicha recta y superponerla en el grafico original.

Para definir la variable E que resuma el error de representación de cada país, primero debemos entender qué es este error. El error de representación es la distancia entre el punto original y su proyección en la línea. Esta distancia es la longitud de la línea perpendicular (ortogonal) desde el punto hasta la línea de proyección

#### Paso 1: Encontrar la dirección ortogonal a la recta elegida en b).

Dado que la recta elegida en b) tiene una pendiente de 1 (y=x), su vector de direccion es [1,1]. Un vector ortogonal a este es [-1,1] o su inveso [1,-1]. Vamos a usar el primero [-1,1]

#### Paso 2: Proyectar los datos en la direccion ortogonal. 

La proyeccion de cada punto de X en la direccion ortogonal se obtiene colaculadno el producto escalar entre el punto y el vector ortogonal.

```{r}

a_ortogonal = c(-1,1)

norma_a_ortogonal = sqrt(sum(a_ortogonal^2))

a_ortogonal_nomralizado = a_ortogonal / norma_a_ortogonal

proyecciones_ortogonales = X %*% a_ortogonal_nomralizado

```

#### Paso 3: Calcular el error E.

El error E para cada punto es la distancia euclidiana entre el punto original y su proyección ortogonal.

```{r}
E = sqrt(rowSums((X - matrix(rep(proyecciones_ortogonales, each=2), ncol=2))^2))

```

#### Paso 4: Encontrar la pendiente y ordenada al origen de la recta ortogonal.
La pendiente de la recta ortogonal es el negativo del recíproco de la pendiente original. Dado que la pendiente original es 1, la pendiente de la recta ortogonal es -1. La ordenada al origen sigue siendo 0, ya que la recta pasa por el origen.

#### Paso 5: Superponer la recta ortogonal en el gráfico original.

```{r}
plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados con recta ortogonal", xlab="Imports", ylab="Exports", pch=5, col="blue")
abline(a=0, b=1, col="red", lwd=2)  # Recta original
abline(a=0, b=-1, col="green", lwd=2)  # Recta ortogonal

```

Conceptualmente: La recta ortogonal representa la dirección en la que se encuentra el mayor error de representación para cada punto. Al proyectar los puntos en esta dirección, estamos midiendo cuánto se desvían de la línea original en términos perpendiculares.


### g. Armar una matriz T ortogonal de 2x2, de modo que el producto XT devuelva un nuevo data set con las variables Y y E en dos columnas.

Para obtener una matriz T ortogonal de 2x2 que, al multiplicarla por X, nos de un nuevo dataset con las variables Y y E en dos columnas, debemos coinsderar lo siguiente:

1. La priemra columna de T debe ser el vector normalizado que define la direccion de proyeccion Y. En nuestro caso, es la direccion de la recta y = x, que es [1,1] normalizado. 

2. La segunda columna de T debe ser el vector normalizado que define la direccion de proyeccion E. En nuestro caso, es la direccion ortogonal de la recta y = x, que es [-1, 1] normalizado. 

#### Paso 1: Definir y normalizar los vectores de dirección.

```{r}
#Vector Y 
a_Y  = c(1,1)

norma_a_Y = sqrt(sum(a_Y^2))

a_Y_nomralizado = a_Y / norma_a_Y

#Vetor E 
a_E  = c(-1,1)

norma_a_E = sqrt(sum(a_E^2))

a_E_nomralizado = a_E / norma_a_E
```

#### Paso 2: Construir la matriz T ortogonal. 

```{r}
T = cbind(a_Y_nomralizado, a_E_nomralizado)

T
```

#### Paso 3: Multiplicar X por T para obtener el nuevo dataset.

```{r}

YE = X %*% T

colnames(YE) = c("Y", "E")

YE = as.data.frame(YE)

YE

```

Conceptualmente: La matriz T es una matriz de transformación que rota y proyecta los puntos en X a lo largo de las direcciones definidas por Y y E. Al multiplicar X por T, estamos transformando cada punto en X a este nuevo espacio definido por Y y E.

### h. Hacer un scatterplot de las variables Y y E. ¿Que observa? Estimar la matriz de varianzas y covarianzas de los nuevos datos.

```{r}
plot(x = YE$Y, y = YE$E, main="Scatter plot de E y Y", xlab="Y", ylab="E", pch=5, col="blue")
```

```{r}
#VAR
var_Y = var(YE$Y)
var_Y

var_E = var(YE$E)
var_E


#COV

cov_YE = cov(x = YE$Y, y = YE$E)
cov_YE
```
Conceptualmente: La varianza de Y es mayor que la de E, lo que indica que la dirección de Y captura la mayor parte de la variación en los datos. La covarianza cercana a cero entre Y y E confirma que las proyecciones son ortogonales y, por lo tanto, independientes entre sí.

### i. Repetir los incisos anteriores usando una nueva direccion de proyeccion que no sea ortogonal a a. ¿Que observa?

Se elige una direccionde pendiente 2 ( y = 2x).

```{r}
# Definir y normalizar la nueva dirección de proyección.
a_nuevo <- c(1, 2)
norma_a_nuevo <- sqrt(sum(a_nuevo^2))
a_nuevo_normalizado <- a_nuevo / norma_a_nuevo

```

```{r}
#Proyectar los datos en la nueva dirección.
proyecciones_nuevo <- X %*% a_nuevo_normalizado

```

```{r}
#Calcular el error E para la nueva dirección.
E_nuevo <- sqrt(rowSums((X - matrix(rep(proyecciones_nuevo, each=2), ncol=2))^2))

```

```{r}
#Scatterplot de las nuevas proyecciones y errores.
plot(proyecciones_nuevo, E_nuevo, main="Scatterplot de Y vs E para nueva dirección", xlab="Y", ylab="E", pch=19, col="darkgreen")

```

```{r}
#Estimar la matriz de varianzas y covarianzas para la nueva dirección.
nuevo_dataset_2 <- data.frame(Y=proyecciones_nuevo, E=E_nuevo)
matriz_cov_2 <- cov(nuevo_dataset_2)
print(matriz_cov_2)
```

Observaciones:

Scatterplot: En el scatterplot de las nuevas proyecciones Y vs errores E, es posible que observes una distribución diferente de puntos en comparación con la dirección original. Dependiendo de la estructura de los datos, la variación capturada por la nueva dirección puede ser menor o mayor que la original.

Matriz de Covarianza: La varianza de Y para la nueva dirección te indicará cuánta variación está siendo capturada por esta dirección. Si es significativamente menor que la varianza de Y para la dirección original, indica que esta nueva dirección no es tan "informativa" como la original. La covarianza entre Y y E no será exactamente cero, ya que la nueva dirección no es ortogonal a la original.

Conceptualmente: Al elegir una dirección de proyección que no es ortogonal a la original, estamos explorando diferentes formas de representar los datos. La calidad de esta representación se refleja en la varianza de las proyecciones y en el error de representación. Si la nueva dirección captura menos variación que la original, indica que la dirección original era más adecuada para representar la estructura subyacente de los datos.

### j. Aplicar la estandarizacion multivariante a los datos. Verificar que las distancias eucleudeas en el nuevo espacio corresponden a las distancias de Mahalanobis en el espacio original.

Teoría:

La estandarización multivariante implica transformar cada variable (columna) de un conjunto de datos para que tenga una media de 0 y una desviación estándar de 1. Esto se hace para que todas las variables tengan la misma escala y, por lo tanto, tengan la misma influencia en los análisis multivariantes.

La distancia de Mahalanobis entre dos puntos en un espacio multivariante es una medida de distancia que tiene en cuenta la correlación entre las variables y la escala de las variables. Es una generalización de la distancia euclidiana que toma en cuenta la estructura de covarianza de los datos.

Relación entre Distancia Euclidiana y Mahalanobis:
Después de la estandarización multivariante, la matriz de covarianza se convierte en la matriz identidad. Por lo tanto, la distancia de Mahalanobis en el espacio original se convierte en la distancia euclidiana en el espacio estandarizado.


```{r}
X = trade 

X_estand = scale(X)


#Cálculo de la Distancia de Mahalanobis en el espacio original
cov_matrix <- cov(X)
inv_cov_matrix <- solve(cov_matrix)
dist_mahalanobis <- mahalanobis(X, colMeans(X), inv_cov_matrix)
head(dist_mahalanobis)

#Cálculo de la Distancia de Mahalanobis en el espacio original
dist_euclidiana <- dist(X_estand)
head(dist_euclidiana)
```

Al comparar dist_mahalanobis y dist_euclidiana, deberían ser aproximadamente iguales, lo que verifica la relación teórica entre la distancia de Mahalanobis en el espacio original y la distancia euclidiana en el espacio estandarizado.


```{r}
summary(dist_mahalanobis)
summary(as.vector(dist_euclidiana))

```
```{r}
diferencias <- abs(dist_mahalanobis - as.vector(dist_euclidiana))
summary(diferencias)
```

```{r}
hist(diferencias, main="Histograma de Diferencias", xlab="Diferencia", ylab="Frecuencia")
```




