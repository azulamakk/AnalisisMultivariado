v1 <- A[,1]
v2 <- A[,2]
# Verificar ortogonalidad
sum(v1 * v2)
#Los vectores v1 y v2 son ortogonales entre sí (producto escalar cercano a 0). Esto significa que son perpendiculares en el espacio en el que están definidos.
# Verificar ortonormalidad
sum(v1^2)
sum(v2^2)
#Ambos vectores son unitarios. Esto significa que tienen una longitud (o magnitud) de 1.
# Graficar los datos transformados
plot(W_transformed[,1], W_transformed[,2], asp=1, xlab="Dimensión 1", ylab="Dimensión 2", main="Datos Transformados")
# Marcar los puntos donde data$etiqueta es 1
points(W_transformed[data$etiqueta == 1,1], W_transformed[data$etiqueta == 1,2], col="red", pch=19)
install.packages("corrplot")
install.packages("tidyverse")
df_housing = read.csv("C:/Users/Justo/Downloads/archive (3)/Melbourne_housing_FULL.csv")
melbourne_filtrado <- read.csv("~/Universidad/Analisis Multivariado/datasets/melbourne_filtrado.csv")
View(melbourne_filtrado)
df_housing = read.csv("~/Universidad/Analisis Multivariado/datasets/melbourne_filtrado.csv")
df_housing = na.omit(df_housing)
colnames(df_housing)
as.data.frame(sapply(df_housing, class))
library(dplyr)
col_numericas = as.data.frame(sapply(df_housing, is.numeric))
df_housing_numericas = df_housing %>% select(which(col_numericas == TRUE))
head(df_housing_numericas)
df_housing_numericas = df_housing_numericas %>% select(-Longtitude, -Lattitude, -Car)
pairs(df_housing_numericas)
medias_housing = sapply(df_housing_numericas, mean)
matriz_cov_housing = cov(df_housing_numericas)
print(matriz_cov_housing)
x = cor(df_housing_numericas)
x
library(corrplot)
corrplot(x, method = "color", type = "upper")
df_housing_numericas_filtrado = df_housing_numericas %>% filter(Landsize <= 2000, YearBuilt >1300)
pairs(df_housing_numericas_filtrado)
df_housing_numericas_scaled = scale(df_housing_numericas)
boxplot.matrix(df_housing_numericas_scaled)
#"Rooms"        "Price"        "Bedroom2"     "Bathroom"     "Landsize"     "BuildingArea" "YearBuilt"
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
df_housing_numericas_maha = mahalanobis(df_housing_numericas, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas))
trade <- read.delim("~/Universidad/Analisis Multivariado/datasets/trade.txt")
View(trade)
trade = read.delim("~/Universidad/Analisis Multivariado/datasets/trade.txt")
names(trade) = c('X2','X1')
summary(trade)
#centramos los datos(estandarizacion)
trade = scale(trade)
#lo volvemos a transformar en un df luego de estandarizarlo
trade = as.data.frame(trade)
#scatterplot
plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados", xlab="Imports", ylab="Exports", pch=5, col="blue")
#para que funcione hay que correr todo junto
plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados", xlab="Imports", ylab="Exports", pch=5, col="blue")
abline(a=0, b=1, col="red", lwd=2)
# Y = X1 + X2
X = data.matrix(trade)
#matrix x
print(X)
a = c(1,1)
norma_a = sqrt(sum(a^2))
a = a / norma_a
#vector de direccion a normalizado
a
Y <- X %*% a
hist(Y)
var_y = var(Y)
var_y
a_ortogonal = c(-1,1)
norma_a_ortogonal = sqrt(sum(a_ortogonal^2))
a_ortogonal_nomralizado = a_ortogonal / norma_a_ortogonal
proyecciones_ortogonales = X %*% a_ortogonal_nomralizado
E = sqrt(rowSums((X - matrix(rep(proyecciones_ortogonales, each=2), ncol=2))^2))
plot(x = trade$X1, y = trade$X2, main="Scatter plot de datos estandarizados con recta ortogonal", xlab="Imports", ylab="Exports", pch=5, col="blue")
abline(a=0, b=1, col="red", lwd=2)  # Recta original
abline(a=0, b=-1, col="green", lwd=2)  # Recta ortogonal
#Vector Y
a_Y  = c(1,1)
norma_a_Y = sqrt(sum(a_Y^2))
a_Y_nomralizado = a_Y / norma_a_Y
#Vetor E
a_E  = c(-1,1)
norma_a_E = sqrt(sum(a_E^2))
a_E_nomralizado = a_E / norma_a_E
T = cbind(a_Y_nomralizado, a_E_nomralizado)
YE = X %*% T
colnames(YE) = c("Y", "E")
YE = as.data.frame(YE)
YE
plot(x = YE$Y, y = YE$E, main="Scatter plot de E y Y", xlab="Y", ylab="E", pch=5, col="blue")
#VAR
var_Y = var(YE$Y)
var_Y
var_E = var(YE$E)
var_E
#COV
cov_YE = cov(x = YE$Y, y = YE$E)
cov_YE
# Definir y normalizar la nueva dirección de proyección.
a_nuevo <- c(1, 2)
norma_a_nuevo <- sqrt(sum(a_nuevo^2))
a_nuevo_normalizado <- a_nuevo / norma_a_nuevo
#Proyectar los datos en la nueva dirección.
proyecciones_nuevo <- X %*% a_nuevo_normalizado
#Calcular el error E para la nueva dirección.
E_nuevo <- sqrt(rowSums((X - matrix(rep(proyecciones_nuevo, each=2), ncol=2))^2))
#Scatterplot de las nuevas proyecciones y errores.
plot(proyecciones_nuevo, E_nuevo, main="Scatterplot de Y vs E para nueva dirección", xlab="Y", ylab="E", pch=19, col="darkgreen")
#Estimar la matriz de varianzas y covarianzas para la nueva dirección.
nuevo_dataset_2 <- data.frame(Y=proyecciones_nuevo, E=E_nuevo)
matriz_cov_2 <- cov(nuevo_dataset_2)
print(matriz_cov_2)
X = trade
X_estand = scale(X)
#Cálculo de la Distancia de Mahalanobis en el espacio original
cov_matrix <- cov(X)
inv_cov_matrix <- solve(cov_matrix)
dist_mahalanobis <- mahalanobis(X, colMeans(X), inv_cov_matrix)
head(dist_mahalanobis)
#Cálculo de la Distancia de Mahalanobis en el espacio original
dist_euclidiana <- dist(X_estand)
head(dist_euclidiana)
summary(dist_mahalanobis)
summary(as.vector(dist_euclidiana))
diferencias <- abs(dist_mahalanobis - as.vector(dist_euclidiana))
summary(diferencias)
hist(diferencias, main="Histograma de Diferencias", xlab="Diferencia", ylab="Frecuencia")
#Librerias
library(tidyverse)
#Plot tridimensional (plotly)
plot3d <- function(X,col=NULL,id=NULL, size=6, cube=TRUE){
library(plotly)
library(RColorBrewer)
n <- nrow(X)
p <- ncol(X)
data <- data.frame(scale(X, scale=FALSE))
names(data) <- c('x1','x2','x3')
if(is.null(col)==TRUE){
data$col <- rep('black',n)
} else {
data$col <-col}
if(is.null(id)==TRUE){
data$id<-1:n
} else {data$id <- id}
fig <- plot_ly(data,
x = ~data[,1], y = ~data[,2], z = ~data[,3],
colors = brewer.pal(p,'Set1'), text=~id,
marker = list(size=size))
fig <- fig %>% add_markers(color = ~col)
fig <- fig %>% layout(scene = list(xaxis = list(title = colnames(X)[1],
range = c(min(data$x1),max(data$x1))),
yaxis = list(title = colnames(X)[2],
range = c(min(data$x2),max(data$x2))),
zaxis = list(title = colnames(X)[3],
range = c(min(data$x3),max(data$x3))),
aspectmode = ifelse(cube==TRUE,'cube','auto')))
fig
}
#Scatter de pares
scatter_pares <- function(data){
panel.hist <- function(x, ...)
{
usr <- par("usr")
par(usr = c(usr[1:2], 0, 1.5) )
h <- hist(x, plot = FALSE)
breaks <- h$breaks; nB <- length(breaks)
y <- h$counts; y <- y/max(y)
rect(breaks[-nB], 0, breaks[-1], y, col = "lightblue", ...)
}
pairs(data, lower.panel = panel.smooth,
upper.panel= NULL, pch=20, lwd=2, diag.panel = panel.hist,
cex.labels = 1.3)
}
#source('librerias y funciones.R') ACA LLLAMA AL ARCHIVO CON LAS FUNCIONES
data = read.csv("C:/Users/Justo/Downloads/mercurio.csv")
mercurio <- read.csv("~/Universidad/Analisis Multivariado/datasets/mercurio.csv")
View(mercurio)
df_housing = read.csv("~/Universidad/Analisis Multivariado/datasets/melbourne_filtrado.csv")
df_housing = na.omit(df_housing)
colnames(df_housing)
as.data.frame(sapply(df_housing, class))
library(dplyr)
col_numericas = as.data.frame(sapply(df_housing, is.numeric))
df_housing_numericas = df_housing %>% select(which(col_numericas == TRUE))
head(df_housing_numericas)
df_housing_numericas = df_housing_numericas %>% select(-Longtitude, -Lattitude, -Car)
pairs(df_housing_numericas)
medias_housing = sapply(df_housing_numericas, mean)
matriz_cov_housing = cov(df_housing_numericas)
print(matriz_cov_housing)
x = cor(df_housing_numericas)
x
library(corrplot)
corrplot(x, method = "color", type = "upper")
df_housing_numericas_filtrado = df_housing_numericas %>% filter(Landsize <= 2000, YearBuilt >1300)
pairs(df_housing_numericas_filtrado)
df_housing_numericas_scaled = scale(df_housing_numericas)
boxplot.matrix(df_housing_numericas_scaled)
#"Rooms"        "Price"        "Bedroom2"     "Bathroom"     "Landsize"     "BuildingArea" "YearBuilt"
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
churn_mini <- read.csv("~/Universidad/Analisis Multivariado/datasets/churn_mini.csv")
View(churn_mini)
constructora <- read.table("~/Universidad/Analisis Multivariado/datasets/constructora.txt", quote="\"", comment.char="")
View(constructora)
constructora <- read.csv("~/Universidad/Analisis Multivariado/datasets/constructora.txt", sep="")
View(constructora)
df_housing = read.csv("~/Universidad/Analisis Multivariado/datasets/melbourne_filtrado.csv")
df_housing = na.omit(df_housing)
colnames(df_housing)
as.data.frame(sapply(df_housing, class))
library(dplyr)
col_numericas = as.data.frame(sapply(df_housing, is.numeric))
df_housing_numericas = df_housing %>% select(which(col_numericas == TRUE))
head(df_housing_numericas)
df_housing_numericas = df_housing_numericas %>% select(-Longtitude, -Lattitude, -Car)
pairs(df_housing_numericas)
medias_housing = sapply(df_housing_numericas, mean)
matriz_cov_housing = cov(df_housing_numericas)
print(matriz_cov_housing)
x = cor(df_housing_numericas)
x
library(corrplot)
corrplot(x, method = "color", type = "upper")
df_housing_numericas_filtrado = df_housing_numericas %>% filter(Landsize <= 2000, YearBuilt >1300)
pairs(df_housing_numericas_filtrado)
df_housing_numericas_scaled = scale(df_housing_numericas)
boxplot.matrix(df_housing_numericas_scaled)
#"Rooms"        "Price"        "Bedroom2"     "Bathroom"     "Landsize"     "BuildingArea" "YearBuilt"
df_housing_numericas_maha = mahalanobis(df_housing_numericas_scaled, center = colMeans(df_housing_numericas_scaled), cov = cov(df_housing_numericas_scaled))
#source('librerias y funciones.R') ACA LLLAMA AL ARCHIVO CON LAS FUNCIONES
data = read.csv("~/Universidad/Analisis Multivariado/datasets/mercurio.csv")
head(data)
plot3d(data[,1:3], col = as.factor(data$etiqueta))
data$mahalnobis<- mahalanobis(data, colMeans(data), cov(data))
n = ncol(data)
data$pvalue_outlier = 1 - pchisq(data$mahalnobis, df = (n-1))
options(scipen=999)
data %>% filter(pvalue_outlier < 0.05))
data$mahalnobis<- mahalanobis(data, colMeans(data), cov(data))
n = ncol(data)
data$pvalue_outlier = 1 - pchisq(data$mahalnobis, df = (n-1))
options(scipen=999)
data %>% filter(pvalue_outlier < 0.05)
#Los puntos que se muestren no pertenecen a la distribucion de nuestros datos (son outliers multivaraidos al 95% de confianza)
X <- data[,1:3]
plot3d(X, col = as.factor(data$etiqueta))
scatter_pares(X)
data_tranf <- data.frame('w1' = sqrt(data$alcalinidad),
'w2' = sqrt(data$clorofila),
'w3' = log(data$mercurio))
plot3d(data_tranf, col = as.factor(data$etiqueta))
scatter_pares(data_tranf)
library(corrplot)
# correlation matrix
# data og
d = data[,1:3]
M<-cor(d)
# data transf
MT<-cor(data_tranf)
# # visualizing correlogram
# # as circle
# corrplot(M, method="circle")
#
# # as pie
# corrplot(M, method="pie")
#
# # as colour
# corrplot(M, method="color")
# Set up a 1x2 matrix of plots
par(mfrow=c(1,2))
# as number
corrplot(M, method="number")
corrplot(MT, method="number")
# Definir la matriz A
A <- cbind(c(-3/4,-2/3,1/5),c(2/3,-4/5,-1/20))
# Suponiendo que data_tranf es tu dataframe
W <- as.matrix(data_tranf)
# Aplicar la transformación
W_transformed <- W %*% A
dim(W_transformed)
v1 <- A[,1]
v2 <- A[,2]
# Verificar ortogonalidad
sum(v1 * v2)
#Los vectores v1 y v2 son ortogonales entre sí (producto escalar cercano a 0). Esto significa que son perpendiculares en el espacio en el que están definidos.
# Verificar ortonormalidad
sum(v1^2)
sum(v2^2)
#Ambos vectores son unitarios. Esto significa que tienen una longitud (o magnitud) de 1.
# Graficar los datos transformados
plot(W_transformed[,1], W_transformed[,2], asp=1, xlab="Dimensión 1", ylab="Dimensión 2", main="Datos Transformados")
# Marcar los puntos donde data$etiqueta es 1
points(W_transformed[data$etiqueta == 1,1], W_transformed[data$etiqueta == 1,2], col="red", pch=19)
data = read.csv("~/Universidad/Analisis Multivariado/datasets/constructora.txt", sep="")
data = data[,-1]
data
#centrado con media cero y desv 1
data = scale(data)
#scatter_pares(data)
pairs(data)
#matriz de covarianza
matriz_cov <- cov(data)
#calculo de autovalores y autovectores
eigen_result = eigen(matriz_cov)
#obteneoms los dos primeros autovectores
autovectores = eigen_result$vectors[ , 1:2]
# Transformación lineal para obtener los scores en dimensión 2
scores = data %*% autovectores
# Graficar los scores
plot(scores, xlab="Componente Principal 1", ylab="Componente Principal 2", main="Gráfico de Scores PCA")
# Visualizar los autovectores
print(autovectores)
# Matriz de covarianzas de los componentes principales
matriz_cov_PC <- diag(eigen_result$values)
matriz_cov
matriz_cov_PC
autovalores <- eigen_result$values
# Proporción de la variabilidad total capturada por las primeras dos componentes
proporcion <- sum(autovalores[1:2]) / sum(autovalores)
print(paste("La proporción de la variabilidad total capturada por las primeras dos componentes es:", round(proporcion, 4)))
# Estandarizar los datos
datos_estandarizados <- scale(data)
# Estimar la matriz de correlación
matriz_corr <- cor(datos_estandarizados)
# Obtener autovalores y autovectores de la matriz de correlación
eigen_corr <- eigen(matriz_corr)
# Autovectores (loadings)
autovectores_corr <- eigen_corr$vectors
# Calcular los scores de las componentes principales
scores_corr <- datos_estandarizados %*% autovectores_corr
# Visualizar los scores
print(scores_corr)
autovalores_corr <- eigen_corr$values
# Proporción de la variabilidad total capturada por las primeras dos componentes
proporcion <- sum(autovalores_corr[1:2]) / sum(autovalores_corr)
print(paste("La proporción de la variabilidad total capturada por las primeras dos componentes es:", round(proporcion, 4)))
#install.packages("caret")
library(caret)
#install.packages("gower")
library(gower)
data <- read.csv("~/Universidad/Analisis Multivariado/datasets/churn_mini.csv")
data <- data[,-1]
# transformo a categoricas las columnas de este tipo
data$Geography <- as.factor(data$Geography)
data$IsActiveMember <- as.factor(data$IsActiveMember)
p = ncol(data) #cantidad de var del dataset
w <- rep(1,p) #peso asigndo a cada var
distancia_gower <- function(data, w){
n <- nrow(data)
dist <- matrix(0,n,n)
p <- ncol(data)
ranges <- sapply(data,
function(x) if (!is.factor(x)) diff(range(x)) else NA)
for(i in 1:n){
for(j in 1:n){
a <- 0
for(k in 1:p){
if(is.factor(data[, k]) == TRUE){
s <- as.numeric((data[i, k] == data[j, k]))
} else {
s <- 1 - abs(data[i, k] - data[j, k])/ranges[k]
}
a <- a + w[k] * s
}
dist[i,j]= 1 - (a / sum(w))
}
}
return(dist)
}
delta <- distancia_gower(data, w = rep(1,7))
delta
mds <- cmdscale(delta, k=2)
?cmdscale
plot(delta, pch = 20, type = 'n')
text(delta, rownames(data))
delta <- distancia_gower((data), w = rep(1,7))
mds <- cmdscale(as.dist(delta), eig = TRUE, k = 2)
plot(mds, pch = 20, type = 'n')
#install.packages("caret")
library(caret)
#install.packages("gower")
library(gower)
data <- read.csv("~/Universidad/Analisis Multivariado/datasets/churn_mini.csv")
data <- data[,-1]
# transformo a categoricas las columnas de este tipo
data$Geography <- as.factor(data$Geography)
data$IsActiveMember <- as.factor(data$IsActiveMember)
p = ncol(data) #cantidad de var del dataset
w <- rep(1,p) #peso asigndo a cada var
distancia_gower <- function(data, w){
n <- nrow(data)
dist <- matrix(0,n,n)
p <- ncol(data)
ranges <- sapply(data,
function(x) if (!is.factor(x)) diff(range(x)) else NA)
for(i in 1:n){
for(j in 1:n){
a <- 0
for(k in 1:p){
if(is.factor(data[, k]) == TRUE){
s <- as.numeric((data[i, k] == data[j, k]))
} else {
s <- 1 - abs(data[i, k] - data[j, k])/ranges[k]
}
a <- a + w[k] * s
}
dist[i,j]= 1 - (a / sum(w))
}
}
return(dist)
}
delta <- distancia_gower(data, w = rep(1,7))
delta
mds <- cmdscale(delta, k=2)
?cmdscale
plot(delta, pch = 20, type = 'n')
text(delta, rownames(data))
delta <- distancia_gower((data), w = rep(1,7))
mds <- cmdscale(as.dist(delta), eig = TRUE, k = 2)
plot(mds, pch = 20, type = 'n')
matriz_cov_mds <- diag(mds$eig)
matriz_cov = cov(delta)
matriz_cov_PC
autovalores <- mds$eig
# Proporción de la distancica total capturada por las primeras dos componentes
proporcion <- sum(autovalores[1:2]) / sum(autovalores)
mds$eigen
mds <- cmdscale(as.dist(distancia_gower(data, w = c(1,100,1,1,1,1,1))))
plot(mds, pch = 20, type = 'n')
text(mds, rownames(data), col ='firebrick')
cov(mds)
mds
heatmap(delta, Rowv = NA, Colv = NA)
heatmap((mds))
plot3d(cmdscale(as.dist(delta), k=3),
cube = FALSE)
#----b) Calcular las distancias de Mahalanobis de los datos a la media y
# estudiar el ajuste con una distribución Chi-Cuadrado de n – 1 grados de libertad.
data$mahalnobis<- mahalanobis(data, colMeans(data), cov(data))
data <- read.csv("~/Universidad/Analisis Multivariado/datasets/mercurio.csv")
plot3d <- function(X,col=NULL,id=NULL, size=6, cube=TRUE){
library(plotly)
library(RColorBrewer)
n <- nrow(X)
p <- ncol(X)
data <- data.frame(scale(X, scale=FALSE))
names(data) <- c('x1','x2','x3')
if(is.null(col)==TRUE){
data$col <- rep('black',n)
} else {
data$col <-col}
if(is.null(id)==TRUE){
data$id<-1:n
} else {data$id <- id}
fig <- plot_ly(data,
x = ~data[,1], y = ~data[,2], z = ~data[,3],
colors = brewer.pal(p,'Set1'), text=~id,
marker = list(size=size))
fig <- fig %>% add_markers(color = ~col)
fig <- fig %>% layout(scene = list(xaxis = list(title = colnames(X)[1],
range = c(min(data$x1),max(data$x1))),
yaxis = list(title = colnames(X)[2],
range = c(min(data$x2),max(data$x2))),
zaxis = list(title = colnames(X)[3],
range = c(min(data$x3),max(data$x3))),
aspectmode = ifelse(cube==TRUE,'cube','auto')))
fig
}
#----a)Explorar la visualización en tres dimensiones de los datos usando la
# librería plotly. Marcar las observaciones etiquetadas con distinto color.
# ¿Que observa sobre el ajuste de los datos a una distribución Normal?
# ¿Presentan los datos una estructura lineal?
X <- data[,1:3]
library(plotly)
plot3d(X, col=as.factor(data$etiqueta))
#----b) Calcular las distancias de Mahalanobis de los datos a la media y
# estudiar el ajuste con una distribución Chi-Cuadrado de n – 1 grados de libertad.
data$mahalnobis<- mahalanobis(data, colMeans(data), cov(data))
n = ncol(data)
data$pvalue_outlier = 1 - pchisq(data$mahalnobis, df=n-1)
options(scipen=999)
data %>% filter(pvalue_outlier < 0.05)
data$mahalnobis<- mahalanobis(data, colMeans(data), cov(data))
n = ncol(data)
data$pvalue_outlier = 1 - pchisq(data$mahalnobis, df=n-1)
options(scipen=999)
data %>% filter(pvalue_outlier < 0.05)
mahala = mahalanobis(data,colMeans(data),cov(data))
plot(density(mahala),col = "purple",main = "Comparación Mahalanobis y Chi-Cuadrado")
curve(dchisq(x,ncol(data)-1), add=TRUE)
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
View(drogas)
data <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
data <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
View(drogas)
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
data <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
datos <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
datos <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
data <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
datos <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
View(drogas)
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
drogas <- read.csv("~/Universidad/Analisis Multivariado/Parcial/drogas.csv")
drogas
